{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc0fe2b-88b2-4b66-be06-d495222db10c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----------+\n| id| name|state|       city|\n+---+-----+-----+-----------+\n|  1| John|   NY|   New York|\n|  2|Alice|   CA|Los Angeles|\n|  3|  Bob|   IL|    Chicago|\n+---+-----+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming 'spark' is your SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, \"John\", {\"city\": \"New York\", \"state\": \"NY\"}),\n",
    "        (2, \"Alice\", {\"city\": \"Los Angeles\", \"state\": \"CA\"}),\n",
    "        (3, \"Bob\", {\"city\": \"Chicago\", \"state\": \"IL\"})]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"address\"])\n",
    "\n",
    "# Selecting required columns\n",
    "result_df = df.select(\"id\", \"name\", col(\"address.state\").alias(\"state\"), col(\"address.city\").alias(\"city\"))\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69adee70-72ec-439e-b35f-f48de44a7ead",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----------+-----+\n| id| name|salary|       city|state|\n+---+-----+------+-----------+-----+\n|  1| John| 50000|   New York|   NY|\n|  2|Alice| 60000|Los Angeles|   CA|\n|  3|  Bob| 70000|    Chicago|   IL|\n+---+-----+------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Assuming 'spark' is your SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [(1, \"John\", 50000, (\"New York\", \"NY\")),\n",
    "        (2, \"Alice\", 60000, (\"Los Angeles\", \"CA\")),\n",
    "        (3, \"Bob\", 70000, (\"Chicago\", \"IL\"))]\n",
    "\n",
    "# Create DataFrame with specified schema\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Selecting required columns\n",
    "result_df = df.select(\"id\", \"name\", \"salary\", \"address.city\", \"address.state\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ef3a85-76f5-4754-93ce-c5680f39ec34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+\n| id|       city| name|\n+---+-----------+-----+\n|  1|   New York| John|\n|  2|Los Angeles|Alice|\n|  3|    Chicago|  Bob|\n|  4|    Houston| null|\n|  5|     Boston| null|\n+---+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Assuming 'spark' is your SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data for small DataFrame\n",
    "small_data = [(1, \"John\"),\n",
    "              (2, \"Alice\"),\n",
    "              (3, \"Bob\")]\n",
    "\n",
    "# Sample data for large DataFrame\n",
    "large_data = [(1, \"New York\"),\n",
    "              (2, \"Los Angeles\"),\n",
    "              (3, \"Chicago\"),\n",
    "              (4, \"Houston\"),\n",
    "              (5, \"Boston\")]\n",
    "\n",
    "# Create DataFrames\n",
    "small_df = spark.createDataFrame(small_data, [\"id\", \"name\"])\n",
    "large_df = spark.createDataFrame(large_data, [\"id\", \"city\"])\n",
    "\n",
    "# Perform broadcast join\n",
    "joined_df = large_df.join(broadcast(small_df), \"id\", \"left\")\n",
    "\n",
    "# Show the result\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2de1492-49d4-41d9-a0ac-7c233599f3f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Prapare data \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "# Inner join\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99d428f8-ecb2-4bfc-9499-e295f30acbc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DF_select_structType and Broadcast-Join and Join",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
